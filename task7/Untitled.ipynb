{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一、卷积运算的定义、动机（稀疏权重、参数共享、等变表示）\n",
    "卷积的定义：卷积是对两个实变函数的一种数学运算。\n",
    "卷积运算通常用星号表示：s(t)=(x∗w)(t)\n",
    "在卷积网络的术语中，卷积的第一个参数（函数 x）通常叫做输入(input)，第二个参数(函数 w)叫做核函数(kernel function)，输出被称作特征映射(feature map)。\n",
    "\n",
    "在机器学习的应用中，输入通常是多维数组的数据，而核通常是由学习算法优化得到的多维数组的参数。我们通常假设在存储了数值的有限点集以外，卷积函数的值都为零，因而我们可以通过对有限个数组元素的求和来计算卷积。卷积运算通常会在多个维度上进行。\n",
    "\n",
    "#### 卷积的动机：\n",
    "卷积运算通过三个重要的思想来帮助改进机器学习系统： 稀疏交互(sparse interactions)、参数共享(parameter sharing)、等变表示(equivariant representa-tions)。另外，卷积提供了一种处理大小可变的输入的方法。\n",
    "\n",
    "    1）稀疏交互：传统的神经网络使用矩阵乘法来建立输入与输出的连接关系，每一个输出单元与每一个输入单元都产生交互。然而，卷积网络具有稀疏交互(sparse interactions)的特征，这是通过使核的大小远小于输入的大小来达到的。\n",
    "\n",
    "    如果有 m 个输入和 n 个输出，那么矩阵乘法需要 m×n个参数并且相应算法的时间复杂度为 O(m×n)。如果我们限制每一个输出拥有的连接数为 k，那么稀疏的连接方法只需要 k×n 个参数以及O(k×n) 的运行时间。在实际应用中，只需保持 k 比 m小几个数量级，就能在机器学习的任务中取得好的表现。\n",
    "\n",
    "    2）参数共享：参数共享(parameter sharing)是指在一个模型的多个函数中使用相同的参数。\n",
    "\n",
    "    在传统的神经网络中，当计算一层的输出时，权重矩阵的每一个元素只使用一次。而在卷积神经网络中，核的每一个元素都作用在输入的每一位置上（是否考虑边界像素取决于对边界决策的设计）。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。\n",
    "\n",
    "    3）等变表示：如果一个函数满足输入改变，输出也以同样的方式改变这一性质，我们就说它是等变(equivariant)的。特别地，如果函数 f(x) 与 g(x)满足 f(g(x))=g(f(x))， 我们就说 f(x) 对于变换 g 具有等变性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 池化运算的定义、种类（最大池化、平均池化等）\n",
    "    3.1 池化作用\n",
    "     池化是缩小高、长方向上的空间的运算。pooling的结果是使得特征减少，参数减少，但pooling的目的并不仅在于此。pooling目的是为了保持某种不变性（旋转、平移、伸缩等）。\n",
    "\n",
    "    3.2 池化分类\n",
    "      常用的有mean-pooling，max-pooling和Stochastic-pooling三种。mean-pooling，即对邻域内特征点只求平均，max-pooling，即对邻域内特征点取最大。根据相关理论，特征提取的误差主要来自两个方面：（1）邻域大小受限造成的估计值方差增大；（2）卷积层参数误差造成估计均值的偏移。\n",
    "\n",
    "    mean-pooling能减小第一种误差（邻域大小受限造成的估计值方差增大），更多的保留图像的背景信息，\n",
    "\n",
    "    max-pooling能减小第二种误差（卷积层参数误差造成估计均值的偏移），更多的保留纹理信息。\n",
    "\n",
    "    Stochastic-pooling则介于两者之间，通过对像素点按照数值大小赋予概率，再按照概率进行亚采样，在平均意义上，与mean-pooling近似，在局部意义上，则服从max-pooling的准则。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from p7_TextCNN_model import TextCNN\n",
    "from data_util import create_vocabulary,load_data_multilabel\n",
    "import os\n",
    "import word2vec\n",
    " \n",
    "#configuration\n",
    "FLAGS=tf.app.flags.FLAGS\n",
    " \n",
    "tf.app.flags.DEFINE_string(\"traning_data_path\",\"../data/sample_multiple_label.txt\",\"path of traning data.\") #sample_multiple_label.txt-->train_label_single100_merge\n",
    "tf.app.flags.DEFINE_integer(\"vocab_size\",100000,\"maximum vocab size.\")\n",
    " \n",
    "tf.app.flags.DEFINE_float(\"learning_rate\",0.0003,\"learning rate\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 64, \"Batch size for training/evaluating.\") #批处理的大小 32-->128\n",
    "tf.app.flags.DEFINE_integer(\"decay_steps\", 1000, \"how many steps before decay learning rate.\") #6000批处理的大小 32-->128\n",
    "tf.app.flags.DEFINE_float(\"decay_rate\", 1.0, \"Rate of decay for learning rate.\") #0.65一次衰减多少\n",
    "tf.app.flags.DEFINE_string(\"ckpt_dir\",\"text_cnn_title_desc_checkpoint/\",\"checkpoint location for the model\")\n",
    "tf.app.flags.DEFINE_integer(\"sentence_len\",100,\"max sentence length\")\n",
    "tf.app.flags.DEFINE_integer(\"embed_size\",128,\"embedding size\")\n",
    "tf.app.flags.DEFINE_boolean(\"is_training\",True,\"is traning.true:tranining,false:testing/inference\")\n",
    "tf.app.flags.DEFINE_integer(\"num_epochs\",10,\"number of epochs to run.\")\n",
    "tf.app.flags.DEFINE_integer(\"validate_every\", 1, \"Validate every validate_every epochs.\") #每10轮做一次验证\n",
    "tf.app.flags.DEFINE_boolean(\"use_embedding\",False,\"whether to use embedding or not.\")\n",
    "tf.app.flags.DEFINE_integer(\"num_filters\", 128, \"number of filters\") #256--->512\n",
    "tf.app.flags.DEFINE_string(\"word2vec_model_path\",\"word2vec-title-desc.bin\",\"word2vec's vocabulary and vectors\")\n",
    "tf.app.flags.DEFINE_string(\"name_scope\",\"cnn\",\"name scope value.\")\n",
    "tf.app.flags.DEFINE_boolean(\"multi_label_flag\",True,\"use multi label or single label.\")\n",
    "filter_sizes=[6,7,8]\n",
    " \n",
    "#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\n",
    "def main(_):\n",
    "    trainX, trainY, testX, testY = None, None, None, None\n",
    "    vocabulary_word2index, vocabulary_index2word, vocabulary_label2index, vocabulary_index2label= create_vocabulary(FLAGS.traning_data_path,FLAGS.vocab_size,name_scope=FLAGS.name_scope)\n",
    "    vocab_size = len(vocabulary_word2index);print(\"cnn_model.vocab_size:\",vocab_size);num_classes=len(vocabulary_index2label);print(\"num_classes:\",num_classes)\n",
    "    train, test= load_data_multilabel(FLAGS.traning_data_path,vocabulary_word2index, vocabulary_label2index,FLAGS.sentence_len)\n",
    "    trainX, trainY = train\n",
    "    testX, testY = test\n",
    "    #print some message for debug purpose\n",
    "    print(\"length of training data:\",len(trainX),\";length of validation data:\",len(testX))\n",
    "    print(\"trainX[0]:\", trainX[0]);\n",
    "    print(\"trainY[0]:\", trainY[0])\n",
    "    train_y_short = get_target_label_short(trainY[0])\n",
    "    print(\"train_y_short:\", train_y_short)\n",
    " \n",
    "    #2.create session.\n",
    "    config=tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        #Instantiate Model\n",
    "        textCNN=TextCNN(filter_sizes,FLAGS.num_filters,num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps,\n",
    "                        FLAGS.decay_rate,FLAGS.sentence_len,vocab_size,FLAGS.embed_size,FLAGS.is_training,multi_label_flag=FLAGS.multi_label_flag)\n",
    "        #Initialize Save\n",
    "        saver=tf.train.Saver()\n",
    "        if os.path.exists(FLAGS.ckpt_dir+\"checkpoint\"):\n",
    "            print(\"Restoring Variables from Checkpoint.\")\n",
    "            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n",
    "            #for i in range(3): #decay learning rate if necessary.\n",
    "            #    print(i,\"Going to decay learning rate by half.\")\n",
    "            #    sess.run(textCNN.learning_rate_decay_half_op)\n",
    "        else:\n",
    "            print('Initializing Variables')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            if FLAGS.use_embedding: #load pre-trained word embedding\n",
    "                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, textCNN,FLAGS.word2vec_model_path)\n",
    "        curr_epoch=sess.run(textCNN.epoch_step)\n",
    "        #3.feed data & training\n",
    "        number_of_training_data=len(trainX)\n",
    "        batch_size=FLAGS.batch_size\n",
    "        iteration=0\n",
    "        for epoch in range(curr_epoch,FLAGS.num_epochs):\n",
    "            loss, counter =  0.0, 0\n",
    "            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n",
    "                iteration=iteration+1\n",
    "                if epoch==0 and counter==0:\n",
    "                    print(\"trainX[start:end]:\",trainX[start:end])\n",
    "                feed_dict = {textCNN.input_x: trainX[start:end],textCNN.dropout_keep_prob: 0.5,textCNN.iter: iteration,textCNN.tst: not FLAGS.is_training}\n",
    "                if not FLAGS.multi_label_flag:\n",
    "                    feed_dict[textCNN.input_y] = trainY[start:end]\n",
    "                else:\n",
    "                    feed_dict[textCNN.input_y_multilabel]=trainY[start:end]\n",
    "                curr_loss,lr,_,_=sess.run([textCNN.loss_val,textCNN.learning_rate,textCNN.update_ema,textCNN.train_op],feed_dict)\n",
    "                loss,counter=loss+curr_loss,counter+1\n",
    "                if counter %50==0:\n",
    "                    print(\"Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tLearning rate:%.5f\" %(epoch,counter,loss/float(counter),lr))\n",
    " \n",
    " \n",
    "                if start%(2000*FLAGS.batch_size)==0: # eval every 3000 steps.\n",
    "                    eval_loss, f1_score, precision, recall = do_eval(sess, textCNN, testX, testY,iteration)\n",
    "                    print(\"Epoch %d Validation Loss:%.3f\\tF1 Score:%.3f\\tPrecision:%.3f\\tRecall:%.3f\" % (epoch, eval_loss, f1_score, precision, recall))\n",
    "                    # save model to checkpoint\n",
    "                    save_path = FLAGS.ckpt_dir + \"model.ckpt\"\n",
    "                    saver.save(sess, save_path, global_step=epoch)\n",
    " \n",
    "            #epoch increment\n",
    "            print(\"going to increment epoch counter....\")\n",
    "            sess.run(textCNN.epoch_increment)\n",
    " \n",
    "            # 4.validation\n",
    "            print(epoch,FLAGS.validate_every,(epoch % FLAGS.validate_every==0))\n",
    "            if epoch % FLAGS.validate_every==0:\n",
    "                eval_loss,f1_score,precision,recall=do_eval(sess,textCNN,testX,testY,iteration)\n",
    "                print(\"Epoch %d Validation Loss:%.3f\\tF1 Score:%.3f\\tPrecision:%.3f\\tRecall:%.3f\" % (epoch,eval_loss,f1_score,precision,recall))\n",
    "                #save model to checkpoint\n",
    "                save_path=FLAGS.ckpt_dir+\"model.ckpt\"\n",
    "                saver.save(sess,save_path,global_step=epoch)\n",
    " \n",
    "        # 5.最后在测试集上做测试，并报告测试准确率 Test\n",
    "        test_loss,_,_,_ = do_eval(sess, textCNN, testX, testY,iteration)\n",
    "        print(\"Test Loss:%.3f\" % ( test_loss))\n",
    "    pass\n",
    " \n",
    " \n",
    "# 在验证集上做验证，报告损失、精确度\n",
    "def do_eval(sess,textCNN,evalX,evalY,iteration):\n",
    "    number_examples=len(evalX)\n",
    "    eval_loss,eval_counter,eval_f1_score,eval_p,eval_r=0.0,0,0.0,0.0,0.0\n",
    "    batch_size=1\n",
    "    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\n",
    "        feed_dict = {textCNN.input_x: evalX[start:end], textCNN.input_y_multilabel:evalY[start:end],textCNN.dropout_keep_prob: 1.0,textCNN.iter: iteration,textCNN.tst: True}\n",
    "        curr_eval_loss, logits= sess.run([textCNN.loss_val,textCNN.logits],feed_dict)#curr_eval_acc--->textCNN.accuracy\n",
    "        label_list_top5 = get_label_using_logits(logits[0])\n",
    "        f1_score,p,r=compute_f1_score(list(label_list_top5), evalY[start:end][0])\n",
    "        eval_loss,eval_counter,eval_f1_score,eval_p,eval_r=eval_loss+curr_eval_loss,eval_counter+1,eval_f1_score+f1_score,eval_p+p,eval_r+r\n",
    "    return eval_loss/float(eval_counter),eval_f1_score/float(eval_counter),eval_p/float(eval_counter),eval_r/float(eval_counter)\n",
    " \n",
    "def compute_f1_score(label_list_top5,eval_y):\n",
    "    \"\"\"\n",
    "    compoute f1_score.\n",
    "    :param logits: [batch_size,label_size]\n",
    "    :param evalY: [batch_size,label_size]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_correct_label=0\n",
    "    eval_y_short=get_target_label_short(eval_y)\n",
    "    for label_predict in label_list_top5:\n",
    "        if label_predict in eval_y_short:\n",
    "            num_correct_label=num_correct_label+1\n",
    "    #P@5=Precision@5\n",
    "    num_labels_predicted=len(label_list_top5)\n",
    "    all_real_labels=len(eval_y_short)\n",
    "    p_5=num_correct_label/num_labels_predicted\n",
    "    #R@5=Recall@5\n",
    "    r_5=num_correct_label/all_real_labels\n",
    "    f1_score=2.0*p_5*r_5/(p_5+r_5+0.000001)\n",
    "    return f1_score,p_5,r_5\n",
    " \n",
    "def get_target_label_short(eval_y):\n",
    "    eval_y_short=[] #will be like:[22,642,1391]\n",
    "    for index,label in enumerate(eval_y):\n",
    "        if label>0:\n",
    "            eval_y_short.append(index)\n",
    "    return eval_y_short\n",
    " \n",
    "#get top5 predicted labels\n",
    "def get_label_using_logits(logits,top_number=5):\n",
    "    index_list=np.argsort(logits)[-top_number:]\n",
    "    index_list=index_list[::-1]\n",
    "    return index_list\n",
    " \n",
    "#统计预测的准确率\n",
    "def calculate_accuracy(labels_predicted, labels,eval_counter):\n",
    "    label_nozero=[]\n",
    "    #print(\"labels:\",labels)\n",
    "    labels=list(labels)\n",
    "    for index,label in enumerate(labels):\n",
    "        if label>0:\n",
    "            label_nozero.append(index)\n",
    "    if eval_counter<2:\n",
    "        print(\"labels_predicted:\",labels_predicted,\" ;labels_nozero:\",label_nozero)\n",
    "    count = 0\n",
    "    label_dict = {x: x for x in label_nozero}\n",
    "    for label_predict in labels_predicted:\n",
    "        flag = label_dict.get(label_predict, None)\n",
    "    if flag is not None:\n",
    "        count = count + 1\n",
    "    return count / len(labels)\n",
    " \n",
    "def assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,textCNN,word2vec_model_path):\n",
    "    print(\"using pre-trained word emebedding.started.word2vec_model_path:\",word2vec_model_path)\n",
    "    word2vec_model = word2vec.load(word2vec_model_path, kind='bin')\n",
    "    word2vec_dict = {}\n",
    "    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n",
    "        word2vec_dict[word] = vector\n",
    "    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\n",
    "    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:'PAD'\n",
    "    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\n",
    "    count_exist = 0;\n",
    "    count_not_exist = 0\n",
    "    for i in range(1, vocab_size):  # loop each word\n",
    "        word = vocabulary_index2word[i]  # get a word\n",
    "        embedding = None\n",
    "        try:\n",
    "            embedding = word2vec_dict[word]  # try to get vector:it is an array.\n",
    "        except Exception:\n",
    "            embedding = None\n",
    "        if embedding is not None:  # the 'word' exist a embedding\n",
    "            word_embedding_2dlist[i] = embedding;\n",
    "            count_exist = count_exist + 1  # assign array to this word.\n",
    "        else:  # no embedding for this word\n",
    "            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n",
    "            count_not_exist = count_not_exist + 1  # init a random value for the word.\n",
    "    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n",
    "    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n",
    "    t_assign_embedding = tf.assign(textCNN.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\n",
    "    sess.run(t_assign_embedding);\n",
    "    print(\"word. exists embedding:\", count_exist, \" ;word not exist embedding:\", count_not_exist)\n",
    "    print(\"using pre-trained word emebedding.ended...\")\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
