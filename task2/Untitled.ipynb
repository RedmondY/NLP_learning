{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本文本处理技能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 分词算法设计中的几个基本原则：\n",
    "\n",
    "1、颗粒度越大越好：用于进行语义分析的文本分词，要求分词结果的颗粒度越大，即单词的字数越多，所能表示的含义越确切，如：“公安局长”可以分为“公安 局长”、“公安局 长”、“公安局长”都算对，但是要用于语义分析，则“公安局长”的分词结果最好（当然前提是所使用的词典中有这个词）\n",
    "\n",
    "2、切分结果中非词典词越少越好，单字字典词数越少越好，这里的“非词典词”就是不包含在词典中的单字，而“单字字典词”指的是可以独立运用的单字，如“的”、“了”、“和”、“你”、“我”、“他”。例如：“技术和服务”，可以分为“技术 和服 务”以及“技术 和 服务”，但“务”字无法独立成词（即词典中没有），但“和”字可以单独成词（词典中要包含），因此“技术 和服 务”有1个非词典词，而“技术 和 服务”有0个非词典词，因此选用后者。\n",
    "\n",
    "3、总体词数越少越好，在相同字数的情况下，总词数越少，说明语义单元越少，那么相对的单个语义单元的权重会越大，因此准确性会越高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词的正向最大: 正向即从前往后取词，从7->1，每次减一个字，直到词典命中或剩下1个单字。\n",
    "\n",
    "分词的逆向最大: 逆向即从后往前取词，其他逻辑和正向相同。\n",
    "\n",
    "分词双向最大匹配法: 正向最大匹配法和逆向最大匹配法，都有其局限性，我举得例子是正向最大匹配法局限性的例子，逆向也同样存在（如：长春药店，逆向切分为“长/春药店”），因此有人又提出了双向最大匹配法，双向最大匹配法。即，两种算法都切一遍，然后根据大颗粒度词越多越好，非词典词和单字词越少越好的原则，选取其中一种分词结果输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词、字符频率统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Python中的collections.Counter模块。\n",
    "词频率统计：第一步分词，然后根据分词后的结果进行词频率统计。\n",
    "基于jieba的实现代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode:今天/是/计划/安排/的/第五天/。\n",
      "Counter({'今天': 1, '是': 1, '计划': 1, '安排': 1, '的': 1, '第五天': 1, '。': 1})\n"
     ]
    }
   ],
   "source": [
    "# 官方文档 https://github.com/fxsjy/jieba\n",
    "import jieba\n",
    "from collections import Counter\n",
    "seg_list=jieba.lcut('今天是计划安排的第五天。', cut_all=False)\n",
    "print(\"Default Mode:\" + \"/\".join(seg_list))\n",
    "\n",
    "# lcut 返回列表\n",
    "\n",
    "WordCount = Counter(seg_list)\n",
    "print(WordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'天': 2, '今': 1, '是': 1, '计': 1, '划': 1, '安': 1, '排': 1, '的': 1, '第': 1, '五': 1, '。': 1})\n"
     ]
    }
   ],
   "source": [
    "txt = '今天是计划安排的第五天。'\n",
    "wordcount = Counter(txt)\n",
    "\n",
    "print(wordcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram模型（考虑句子中单词之间的顺序）\n",
    "\n",
    "当n取1、2、3时，n-gram模型分别称为unigram、bigram、trigram语言模型\n",
    "\n",
    "unigram一元分词，把句子分成一个一个的汉字\n",
    "\n",
    "bigram二元分词，把句子从头到尾每两个字组成一个词语\n",
    "\n",
    "trigram三元分词，把句子从头到尾每三个字组成一个词语\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram(dic, train_data_dic):\n",
    "    all_value_un = 0 \n",
    "    # To get all counts\n",
    "    for i in dic:\n",
    "        all_value_un += dic[i]\n",
    "    # To get the train data dictionary\n",
    "    for i in dic:\n",
    "        value = dic[i] / all_value_un\n",
    "        train_data_dic[i] = value\n",
    "    return train_data_dic\n",
    "\n",
    "# To get the bigram words: use 'zip' to structure\n",
    "def bigrams(words):\n",
    "    return zip(words, words[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本矩阵化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    " \n",
    "# 读取停用词\n",
    "def read_stopword(filename):\n",
    "    stopword = []\n",
    "    fp = open(filename, 'r')\n",
    "    for line in fp.readlines():\n",
    "        stopword.append(line.replace('\\n', ''))\n",
    "    fp.close()\n",
    "    return stopword\n",
    " \n",
    " \n",
    "# 切分数据，并删除停用词\n",
    "def cut_data(data, stopword):\n",
    "    words = []\n",
    "    for content in data['content']:\n",
    "        word = list(jieba.cut(content))\n",
    "        for w in list(set(word) & set(stopword)):\n",
    "            while w in word:\n",
    "                word.remove(w)\n",
    "        words.append(' '.join(word))\n",
    "    data['content'] = words\n",
    "    return data\n",
    " \n",
    " \n",
    "# 获取单词列表\n",
    "def word_list(data):\n",
    "    all_word = []\n",
    "    for word in data['content']:\n",
    "        all_word.extend(word)\n",
    "    all_word = list(set(all_word))\n",
    "    return all_word\n",
    " \n",
    " \n",
    "# 计算文本向量\n",
    "def text_vec(data):\n",
    "    count_vec = CountVectorizer(max_features=300, min_df=2)\n",
    "    count_vec.fit_transform(data['content'])\n",
    "    fea_vec = count_vec.transform(data['content']).toarray()\n",
    "    return fea_vec\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv('./cnews/test.txt', names=['title', 'content'], sep='\\t')  # (10000, 2)\n",
    " \n",
    "    stopword = read_stopword('./cnews/stopword.txt')\n",
    "    data = cut_data(data, stopword)\n",
    " \n",
    "    fea_vec = text_vec(data)\n",
    "    print(fea_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package              Version  \n",
      "-------------------- ---------\n",
      "absl-py              0.7.1    \n",
      "astor                0.8.0    \n",
      "backcall             0.1.0    \n",
      "certifi              2019.6.16\n",
      "colorama             0.4.1    \n",
      "cycler               0.10.0   \n",
      "decorator            4.4.0    \n",
      "gast                 0.2.2    \n",
      "google-pasta         0.1.7    \n",
      "grpcio               1.21.1   \n",
      "h5py                 2.9.0    \n",
      "ipykernel            5.1.1    \n",
      "ipython              7.5.0    \n",
      "ipython-genutils     0.2.0    \n",
      "jedi                 0.13.3   \n",
      "jieba                0.39     \n",
      "joblib               0.13.2   \n",
      "jupyter-client       5.2.4    \n",
      "jupyter-core         4.4.0    \n",
      "Keras-Applications   1.0.8    \n",
      "Keras-Preprocessing  1.1.0    \n",
      "kiwisolver           1.1.0    \n",
      "Markdown             3.1.1    \n",
      "matplotlib           3.1.0    \n",
      "mkl-fft              1.0.12   \n",
      "mkl-random           1.0.2    \n",
      "mkl-service          2.0.2    \n",
      "numpy                1.16.4   \n",
      "parso                0.4.0    \n",
      "pickleshare          0.7.5    \n",
      "pip                  19.1.1   \n",
      "prompt-toolkit       2.0.9    \n",
      "protobuf             3.8.0    \n",
      "Pygments             2.4.2    \n",
      "pyparsing            2.4.0    \n",
      "python-dateutil      2.8.0    \n",
      "pytz                 2019.1   \n",
      "pyzmq                18.0.0   \n",
      "scikit-learn         0.21.2   \n",
      "scipy                1.2.1    \n",
      "setuptools           41.0.1   \n",
      "six                  1.12.0   \n",
      "tensorboard          1.14.0   \n",
      "tensorflow           1.14.0   \n",
      "tensorflow-estimator 1.14.0   \n",
      "termcolor            1.1.0    \n",
      "tornado              6.0.2    \n",
      "traitlets            4.3.2    \n",
      "wcwidth              0.1.7    \n",
      "Werkzeug             0.15.4   \n",
      "wheel                0.33.4   \n",
      "wincertstore         0.2      \n",
      "wrapt                1.11.2   \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
