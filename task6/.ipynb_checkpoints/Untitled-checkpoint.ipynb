{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前馈神经网络:前馈神经网络也称为是深度前馈网络或者多层感知机，它是最基础的深度学习模型。\n",
    "\n",
    "网络层数:链的全长称作模型的深度或者神经网络的层数。\n",
    "\n",
    "输入层:深度前馈网络的第一层\n",
    "\n",
    "隐藏层:因为无法观测到除了输出层以外的那些层的输出，因此那些层被称作隐藏层(hidden layer) \n",
    "\n",
    "输出层:深度前馈网络的最后一层\n",
    "\n",
    "隐藏单元: ？\n",
    "\n",
    "激活函数:激活函数是前馈神经网络具有非线性表达能力的核心因素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机模型是机器学习二分类问题中的一个非常简单的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络\n",
    "num_samples = 128 # N\n",
    "dim_in, dim_hid, dim_out = 1000, 100, 10  # IN H OUT\n",
    "x = torch.randn(num_samples, dim_in)  #  N * IN\n",
    "y = torch.randn(num_samples, dim_out) #  N * OUT\n",
    "\n",
    "w1 = torch.randn(dim_in, dim_hid, requires_grad=True)     # IN * H\n",
    "w2 = torch.randn(dim_hid, dim_out, requires_grad=True)    #  H * OUT\n",
    "\n",
    "eta = 1e-6\n",
    "for i in range(1000):\n",
    "  #Forward pass\n",
    "  h = x @ w1                              # N * H\n",
    "  h_relu = h.clamp(min = 0)               # N * H\n",
    "  y_pred = h_relu @ w2                    # N * OUT\n",
    "  \n",
    "  #Loss\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print('times is {}, loss is {}'.format(i, loss.item()))\n",
    "  loss.backward()\n",
    "  \n",
    "  #Backward pass\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    w1 -= eta * w1.grad #如果写成w1 = w1 - eta * w1.grad就会报错\n",
    "    w2 -= eta * w2.grad\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函数在神经元中非常重要的，是增强网络的非线性表示能力和学习能\n",
    "力的关键，一般需要具备以下几点性质：\n",
    "\n",
    "连续并可导（允许少数点上不可导）的非线性函数。可导的激活函数可以直接利用数值优化的方法来学习网络参数。\n",
    "\n",
    "激活函数及其导函数要尽可能的简单，有利于提高网络计算效率。\n",
    "\n",
    "激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用单位函数g(z)=z作为激活函数 。但如果网络的每一层都是由线性变换组成，则网络作为整体也是线性的。这会降低网络的表达能力，因此线性激活函数较少使用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 修正线性单元（relu）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修正线性单元采用激活函数g(z)=max{0,z}，它和线性单元非常类似，区别在于：修正线性单元在左侧的定义域上输出为零。\n",
    "\n",
    "优点：采用基于梯度的优化算法时，非常易于优化。当修正线性单元处于激活状态时，导数为常数1 ；当修正线性单元处于非激活状态时，导数为常数0 。修正线性单元的二阶导数几乎处处为零。\n",
    "\n",
    "缺点：无法通过基于梯度的方法学习那些使得修正线性单元处于非激活状态的参数，因为此时梯度为零。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sigmoid / tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在引入修正线性单元之前，大多数神经网络使用sigmoid函数g(z)=σ(z)，或者双曲正切函数g(z)=tanh(z) g(z)=tanh(z)作为激活函数。这两个激活函数密切相关，因为tanh(z)=2σ(2z)−1。\n",
    "\n",
    "与修正线性单元不同，sigmoid单元和tanh单元在其大部分定义域内都饱和，仅仅当z在 0 附近才有一个较高的梯度，这会使得基于梯度的学习变得非常困难。因此，现在不鼓励将这两种单元用作前馈神经网络中的激活函数。\n",
    "\n",
    "如果选择了一个合适的代价函数（如对数似然函数）来抵消了sigmoid的饱和性，则这两种单元可以用作输出单元（而不是隐单元）。\n",
    "如果必须选用sigmoid激活函数时，tanh激活函数通常表现更佳。因为tanh函数在 0点附近近似于单位函数g(z)=z。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 正则化常用于缓解模型过拟合。过拟合发生的原因是模型的容量过大，而正则化可以对模型施加某些限制，从而降低模型的有效容量。\n",
    "\n",
    "2. 目前有多种正则化策略。\n",
    "\n",
    "   有些正则化策略是向模型添加额外的约束，如增加对参数的限制。这是对参数的硬约束。\n",
    "   \n",
    "   有些正则化策略是向目标函数增加额外项。这是对参数的软约束。\n",
    "   \n",
    "3. 正则化策略代表了某种先验知识，即：倾向于选择简单的模型。\n",
    "\n",
    "4. 在深度学习中，大多数正则化策略都是基于对参数进行正则化。正则化以偏差的增加来换取方差的减少，而一个有效的正则化能显著降低方差，并且不会过度增加偏差。\n",
    "\n",
    "5. 在深度学习的实际应用中，不要因为害怕过拟合而采用一个小模型，推荐采用一个大模型并使用正则化。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据增强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提高模型泛化能力的一个最直接的方法是采用更多的数据来训练。但是通常在现实任务中，我们拥有的数据量有限。解决该问题的一种方法是：创建一些虚拟的数据用于训练。\n",
    "\n",
    "数据集增强仅仅用于模型的训练，而不是用于模型的预测。即：不能对测试集、验证集执行数据集增强。\n",
    "\n",
    "当比较机器学习算法基准测试的结果时，必须考虑是否采用了数据集增强。通常情况下，人工设计的数据集增强方案可以大大减少模型的泛化误差。当两个模型的泛化性能比较时，应该确保这两个模型使用同一套人工设计的数据集增强方案。\n",
    "\n",
    "注意数据集增强和预处理的区别：数据集增强会产生更多的输入数据，而数据预处理产生的输入数据数量不变。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 噪声添加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有两种添加噪声的策略：输入噪声注入、权重噪声注。\n",
    "\n",
    "  输入噪声注入是将噪声作用于输入的数据集，这也是一种数据集增强方法。对于某些模型，在输入上注入方差极小的噪音等价于对权重施加参数范数正则化（Bishop,1995a,b）。但是输入噪声注入远比简单地收缩参数强大，尤其是噪声被添加到隐单元的输入上时。\n",
    "  \n",
    "  权重噪声注入是将噪音作用于权重。这项技术主要用于循环神经网络。权重噪声注入可以解释为：将权重视作不确定的随机变量（拥有某个概率分布），向权重注入噪声是对该随机变量采样得到的一个随机值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度模型的优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数初始化策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有些优化算法是非迭代的，可以直接解析求解最优解；有些优化算法是迭代的，但是它们是初始值无关的。深度学习不具有这两类性质，通常是迭代的，且与初始值相关。\n",
    "\n",
    "深度学习中，大多数算法都受到初始值的影响。初始值能够决定：算法最终是否收敛、以及收敛时的收敛速度有多快、以及收敛到一个代价函数较高还是较低的值。\n",
    "\n",
    "深度学习中，初始值也会影响泛化误差，而不仅仅是目标函数的最优化。因为如果选择一个不好的初始值，则最优化的结果会落在参数空间的一个较差的区域。此时会导致模型一个较差的泛化能力。\n",
    "\n",
    "目前深度学习中，选择初始化策略是启发式的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 权重初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常权重的初始化是从高斯分布或者均匀分布中挑选出来的值。\n",
    "\n",
    "从高斯分布还是均匀分布中挑选，看起来似乎没有很大差别，实际上也没有被仔细研究。\n",
    "\n",
    "该分布的范围（如均匀分布的上、下限）对优化结果和泛化能力有很大的影响。\n",
    "\n",
    "初始权重的大小很重要，下面的因素决定了权重的初始值的大小：\n",
    "\n",
    "更大的初始权重具有更强的破坏对称性的作用，有助于避免冗余的单元。\n",
    "\n",
    "更大的初始权重也有助于避免梯度消失。\n",
    "\n",
    "更大的初始权重也容易产生梯度爆炸。\n",
    "\n",
    "循环神经网络中，更大的初始权重可能导致混沌现象：对于输入中的很小的扰动非常敏感，从而导致确定性算法给出了随机性结果。\n",
    "\n",
    "关于如何初始化网络，正则化和最优化有两种不同的角度：\n",
    "\n",
    "从最优化角度，建议权重应该足够大，从而能够成功传播信息。\n",
    "\n",
    "从正则化角度，建议权重小一点（如 正则化），从而提高泛化能力。\n",
    "\n",
    "实践中，通常需要将初始权重范围视作超参数。如果计算资源允许，可以将每层权重的初始数值范围设置为一个超参数，然后使用超参数搜索算法来挑选这些超参数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "astText是一种Facebook AI Research在16年开源的一个文本分类器。 其特点就是fast。相对于其它文本分类模型，如SVM，Logistic Regression和neural network等模型，fastText在保持分类效果的同时，大大缩短了训练时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText.FastText as ff\n",
    "classifier = ff.train_supervised(\"data/try_fasttext_train.txt\")\n",
    "model = classifier.save_model('data/try.model') # 保存模型\n",
    "test = classifier.test('data/try_fasttext_test.txt') # 输出测试结果\n",
    "classifier.get_labels() # 输出标签\n",
    "pre = classifier.predict('文本') #输出改文本的预测结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "#训练模型\n",
    "classifier = fasttext.supervised(\"data/try_fasttext_train.txt\",\"data/try_fasttext.model\",label_prefix=\"__label__\")\n",
    "  \n",
    "#load训练好的模型\n",
    "#classifier = fasttext.load_model('data/try_fasttext.model.bin', label_prefix='__label__')\n",
    "  \n",
    "result = classifier.test(\"data/try_fasttext_test.txt\")\n",
    "print(result.precision)\n",
    "print(result.recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
